

## 一、self-attention

### 1.Vector Set as Input

之前是输入一个向量，现在需要输入一个vector set

- 文字处理  （one-hot encoding  和 word embedding）

前者存在维度高，彼此无关联

- 声音信号
- 图
- 分子信息

### 2.Output

输出有三种可能：

- 每一个向量都有一个对应的Label
- 一整个Sequence，只需要输出一个Label
- 机器自己决定最后有多少个Label （seq2seq）

### 3.self-attention 介绍

从结构上看，self-attention会考虑整个sequence的信息，所以FC的Network考虑的不只是一个小范围或小窗口。该结构可以套用多次。

内部实现，是乘一个矩阵W得到相应的向量q、k、v，再利用点积和其他技术(关联性,softmax)计算b

最后可以在输入的部分注入positional encoding的技术，来添加位置信息



再回过头来看`self-attention`和`CNN`的区别，CNN可以看出简化版的self-attention，CNN只考虑receptive field的信息，而在做self-attention的时候，我们是考虑整张图片的信息

`self-attention`和`RNN`的区别：一般RNN只考虑左边的输入，双向RNN需要额外的memory来记录左边的输入，RNN没法平行化，效率比较低



## 二、RNN

- RNN可以deep
- RNN可以双向
- RNN的BP采用BPTT

## 三、LSTM

LSTM有四个输入（分别是输入门，遗忘门，输出门和原始输入），一个输出 。

RNN的训练通常比较困难，因为它的error surface很陡峭或者平坦。
容易产生**梯度爆炸**或者**梯度消失**。

利用LSTM可以解决梯度消失问题。
原因是：
1.Memory和Input是相加的（在RNN中每个时间点的memory会被洗掉）
2.只要forget gate不关，cell的影响始终存在

参考文章：

https://blog.csdn.net/toro180/article/details/125498102

[人人都能看懂的GRU](https://zhuanlan.zhihu.com/p/32481747) 【GRU的 提出目的同LSTM，为了解决长期记忆和反向传播的梯度问题】





