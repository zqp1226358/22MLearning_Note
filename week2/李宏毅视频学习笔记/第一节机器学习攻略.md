## 机器学习攻略

本节主要讲在模型训练的过程中，如何做让模型更好

- **首先检查Training Data的loss**

  如果Training Data的loss已经很大，那么说明它在训练集就没有训练好。

  这时候再考虑两种情况

  - **Model Bias（偏差）**

  > 模型弹性不够大，需要设计一个弹性更大的Model

  - **Optimization** 

  > 梯度下降遇到了问题

  **如何区分是Model Bias还是Optimization Issue？**

  比较不同的模型，先看较小的network的loss是多少，再换一个大模型的网络，看看loss是多少，如果大模型的loss太大，说明可能Optimization遇到了问题

- **检测Testing Data的loss**

  假设经过努力使得training data的loss变小了，那么就可以看看Testing Data的loss，如果loss也很小，则没什么可做的了

  如果不够小，即Training data loss小，testing data loss大，则有可能是overfitting的问题

  - overfitting 过拟合

  >可以增加训练集，给模型增加限制

  - Mismatch

  >还有一种情况是训练集和测试集的分布是不一样的  可见作业HW11



### 数据集分为Train/dev/Test的原因

用Train来训练，用val来验证模型的好坏，这样不会受到test的影响。

其他有N-fold Cross Validation的方式，



### 类神经网络训练不起来怎么办

#### 1.局部最小值和鞍点

如何判断是否是局部最小值和鞍点，（结论）计算一个叫hessian的东西，这个矩阵它所有的eigen value 都是正的，则代表local minima，如果有正有负，则代表在saddle point

loacl minima为假问题。由于深度学习的参数很多，维度很高，从低维的空间来看,是没有路可以走的东西,在高维的空间中是有路可以走的，所以一般在DP中不会出现local minima的问题

saddle point问题可以有许多方法去逃离

#### 2.批次batch和动量momentum

记录最终的结论：

- 由于平行化计算的原因，large batch计算完整一个epoch的时间更短
- small batch的噪声更大，帮助模型走出saddle point，更好的optimize
- small batch的泛化效果更好，（用一个比喻，大的batch size 会倾向走到峡谷里面，而小的batch size 倾向于走到盆地）

**momentum** 是另一个对抗saddle point或者local minima的方法

简单来说就是给梯度下降加了一个动量

#### 3.自动调整学习率 Adaptive learning rate

3.1 loss不下降时，gradient不一定很小

3.2 参数客制化

3.3 Adam

3.4 Learning rate scheduling

#### 4.损失函数可能也有影响

####5.Batchnormalization 批次标准化

  